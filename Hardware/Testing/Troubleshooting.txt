#OS info
apt install -y sosreport
https://github.com/sosreport/sos.git

#GPU and System logs
nvidia-bug-report.sh

#System & NIC logs 
sysinfo-snapshot.py


#FD_200

(1)check for the error message --> (2) go to the folder check for exception to confirm issue check for Error code in (fd_sum/output) to determine the next steps --> if point to GPU or NVs, go to that folder and repeats (2). 
++++++++++
MODS-000000000143 | connectivity       | connectivity | nvlink     | GPU, PCIE, Nvlink, I2C |                        | PCI Express bus error
MODS-000000000143 | gpumem             | gpumem       |            | GPU                    | SXM2_SN_1652xxxxxxxxx  | PEX error threshold exceeded.
gpumem             --->  SXM2_SN_1652xxxxxx

connectivity folder
connectivity folder --> exception (confirm there is an issue) |--> FD_sum (Error Code)             ---> GPU# --> exception   --> |--> FD_sum (Error Code) 
                                                              |--> output (Error Code)                      ---> Error Code      |--> output
                                                              |**--> if >2 fabricmanager or nvlsm 


#GPU issue
 
systemctl stop *nvidia*
ps -ef | grep -i nvidia
lsof | grep -i nvidia

sudo rmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia
lsof /dev/nvidia* | awk '{print $2}' | grep -E '^[0-9]+$' | sort -u | xargs -r kill -9 #none of service using the GPU

#remove driver
echo -n "0000:3b:00.0" > /sys/bus/pci/drivers/nvidia/unbind #NVRM: Continuing with GPU removal for device 0000:3b:00.0 (no longer know the GPU/ /sys/bus/pci/drivers/nvidia)
echo -n "0000:3b:00.0" > /sys/bus/pci/drivers/nvidia/bind

#drop from the pcie
echo 1 | sudo tee /sys/bus/pci/devices/0000\:1a\:00.0/remove
echo 1 | sudo tee /sys/bus/pci/rescan

#status
ls /sys/bus/pci/devices/0000:3b:00.0/power/control /runtime_status /runtime_usage
cat /sys/bus/pci/devices/0000:3b:00.0/power_state /aer_dev_fatal /current_link_width


hexdump -C /sys/bus/pci/devices/0000\:3b\:00.0/config

echo -n "0000:29:00.0" > /sys/bus/pci/drivers/mlx5_core/unbind
echo -n "0000:29:00.0" >  /sys/bus/pci/drivers/mlx5_core/bind

#disable hung task if possible
echo 0 > /proc/sys/kernel/hung_task_timeout_secs


time nvidia-smi
real    0m0.515s
user    0m0.002s
sys     0m0.468s
time nvidia-smi -q | grep -i vbios
real    0m1.133s
user    0m0.028s
sys     0m0.753s




#Check system
Component                           | Details
================================================================================
Vulkan Info                         | None
--------------------------------------------------------------------------------
NVIDIA SMI                          | NVIDIA-SMI version  : 570.133.20
                                    | NVML version        : 570.133
                                    | DRIVER version      : 570.133.20
                                    | CUDA Version        : 12.8
--------------------------------------------------------------------------------
NVIDIA Settings                     | 
                                    | nvidia-settings:  version 575.51.03
                                    |   The NVIDIA Settings tool.
--------------------------------------------------------------------------------
NVIDIA Fabric Manager               | Fabric Manager version is : 570.133.20
--------------------------------------------------------------------------------
NVIDIA Subnet Manager               | None
--------------------------------------------------------------------------------
Mellanox Link                       | mlxlink, mft 4.33.0-169, built on Jul 24 2025, 16:49:46. Git SHA Hash: N/A
--------------------------------------------------------------------------------
InfiniBand Status                   | ibstat BUILD VERSION: 39.0
--------------------------------------------------------------------------------
InfiniBand Network Discovery        | ibnetdiscover BUILD VERSION: 39.0
--------------------------------------------------------------------------------
NVIDIA MSE/NETIR Versions           | None
--------------------------------------------------------------------------------
NVIDIA GPU Details                  | NVIDIA B200, 570.133.20, 183359 MiB, 97.00.88.00.0F, 00000000:1A:00.0, 1651625050300
NVIDIA NIC Details                  | Device      : mlx5_0
                                    | Firmware Ver: 28.42.1280
--------------------------------------------------------------------------------
OS Details                          | Distribution : Ubuntu 22.04.2 LTS
                                    | Kernel       : 5.15.0-139-generic
                                    | Hostname     : b200
                                    | Architecture : x86_64
                                    | Uptime       : up 17 minutes

Loading driver
journalctl -b -0
journalctl -b -1
journalctl -b -2
Oct 15 23:12:30 b200 kernel: [drm] [nvidia-drm] [GPU ID 0x00001a00] Loading driver
Oct 15 23:12:30 b200 kernel: NVRM: nvCheckOkFailedNoLog: Check failed: Generic Error: Invalid state [NV_ERR_INVALID_STATE] (0x00000040) returned from pRmApi->Control(pRmApi, pGpu->hInternalClient, pGpu->hInternalSubdevice, NV2080_CTRL_CMD_NVLINK_GET_POWER_STATE, &powerStatusParams, sizeof(powerStatusParams)) @ gpu_fabric_probe.c:813
Oct 15 23:12:30 b200 kernel: NVRM: nvCheckOkFailedNoLog: Check failed: Generic Error: Invalid state [NV_ERR_INVALID_STATE] (0x00000040) returned from pRmApi->Control(pRmApi, pGpu->hInternalClient, pGpu->hInternalSubdevice, NV2080_CTRL_CMD_NVLINK_GET_POWER_STATE, &powerStatusParams, sizeof(powerStatusParams)) @ gpu_fabric_probe.c:813


dpkg -S $(which sysinfo-snapshot.py)
ofed-scripts: /usr/sbin/sysinfo-snapshot.py
dpkg -S ofed-scripts
apt show ofed-scripts